{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook for final evaluation on the test data set\n",
    "-------------------\n",
    "This notebook is meant to evaluate the final model of your group on the hold-out test set.\n",
    "\n",
    "Fill in the notebook with the data preparation steps needed to generate the features for your model and apply your model to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "################################\n",
    "**Do not change the code in this section!**\n",
    "################################\n",
    "\n",
    "In this section, the metadata file is loaded into a dictionary and the true label of the cylinder bottoms is extracted. The metadata file will be located in \"data/cylinder_bottom_test/meta_data.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cylinder_bottom_test/meta_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load the json file with metadata\u001b[39;00m\n\u001b[0;32m      7\u001b[0m metadata_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cylinder_bottom_test/meta_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m meta_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cylinder_bottom_test/meta_data.json'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# load the json file with metadata\n",
    "metadata_file = 'data/cylinder_bottom_test/meta_data.json'\n",
    "f = open(metadata_file)\n",
    "meta_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def assign_label(part_anomaly):\n",
    "    if part_anomaly >= 1:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def get_anomalies(metadata):\n",
    "    \"\"\"\n",
    "    :param metadata: dictionary imported from meta data file\n",
    "    :return: ground truth dataframe with anomaly label for each part\n",
    "    \"\"\"\n",
    "    # transform dict into pd dataframe and filter only the milling events\n",
    "    metadata_process = pd.json_normalize(metadata, 'process_data', ['part_type', 'part_id'])\n",
    "    metadata_process_milling = metadata_process[metadata_process['name']=='cnc_milling_machine']\n",
    "\n",
    "    # extract anomaly information\n",
    "    anomalies = metadata_process_milling['anomaly']\n",
    "    anomalies = anomalies.reset_index(drop=True)\n",
    "    anomalies = anomalies.astype(int)\n",
    "    # transformation of anomaly information to a binary label\n",
    "    anomalies = anomalies.apply(assign_label)\n",
    "    return anomalies\n",
    "\n",
    "# extract the true labels from the metadata file\n",
    "y_true = get_anomalies(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, you should perform all necessary steps of data preparation. The output of this section shall be a matrix X containing all the features for the ML model.\n",
    "\n",
    "The folders with the recorded data are stored in the path \"/data/cylinder_bottom_test/cnc_milling_machine/process_data/\" and are also described in the metadata filed loaded in the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timeseries Features\n",
    "class TimeFeatures:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    #1 'mean'\n",
    "\n",
    "    #2 'max'\n",
    "\n",
    "    #3 'min'\n",
    "\n",
    "    #4 rms\n",
    "    def rms(x):\n",
    "        #lambda x: np.sqrt(sum(x**2)/x.size)\n",
    "        res = np.sqrt(np.mean(x**2))\n",
    "        return res\n",
    "\n",
    "    #5 abs_mean\n",
    "    def abs_mean(x):\n",
    "        res = np.mean(np.abs(x))\n",
    "        return res\n",
    "\n",
    "    #6 scipy.stats.kurtosis()\n",
    "    def kurtosis(x):\n",
    "        res = scipy.stats.kurtosis(x)\n",
    "        return res\n",
    "\n",
    "    #7 scipy.stats.skew()\n",
    "    def skew(x):\n",
    "        res = scipy.stats.skew(x)\n",
    "        return res\n",
    "\n",
    "    #8 std deviation\n",
    "\n",
    "    #9 coefficient of variation\n",
    "    def coef_var(x):\n",
    "        res = scipy.stats.variation(x)\n",
    "        return res\n",
    "\n",
    "    #9 zero peak to peak: np.ptp (Barsczc2019)\n",
    "    def zptp(x):\n",
    "        res = np.ptp(x)/2\n",
    "        return res\n",
    "\n",
    "    #10 Crest Factor \n",
    "    def crest(x):\n",
    "        res = (np.ptp(x)/2)/TimeFeatures.rms(x)\n",
    "        return res\n",
    "\n",
    "    #11 Impulse Factor (Ahmed2020/Wang2019b)\n",
    "    def impulse_factor(x):\n",
    "        res = x.max()/x.abs().mean()\n",
    "        return res\n",
    "\n",
    "    #12 Margin Factor (Ahmed2020)\n",
    "    def margin_factor(x):\n",
    "        peak= np.ptp(x)/2\n",
    "        res = peak/((np.mean(np.sqrt(np.abs(x))))**2)\n",
    "        return res\n",
    "\n",
    "    #13 Shape Factor (Ahmed2020/Wang2019b)\n",
    "    def shape_factor(x):\n",
    "        rms = np.sqrt(np.mean(x**2))\n",
    "        res = rms/(np.mean(np.abs(x)))\n",
    "        return res\n",
    "\n",
    "    #14 Clearance Factor (Ahmed2020/Wang2019b)\n",
    "    def clearance_factor(x):\n",
    "        res = x.max()/((np.mean(np.sqrt(np.abs(x))))**2)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensor(obj_features, data_paths):\n",
    "    with h5py.File(data_paths, 'r') as hf:\n",
    "        # Access the dataset and column names\n",
    "        data = hf[\"data\"]\n",
    "        column_names = data.attrs[\"column_names\"]\n",
    "        \n",
    "        # Pandas to read the data\n",
    "        df = pd.DataFrame(data, columns=column_names)\n",
    "    # add the word frontside/backside to not overwrite values\n",
    "    file_name = data_paths.split('/')[-1]\n",
    "    first_word = file_name.split('_')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in [\"timestamp\"]:\n",
    "            continue\n",
    "        # Calculate the mean of the column\n",
    "        col_mean = df[col].mean()\n",
    "        # Add the mean value to the dictionary with the column name as the key\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"mean\"] = col_mean\n",
    "        \n",
    "        # Calculate possible time series features\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"rms\"] = TimeFeatures.rms(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"abs_mean\"] = TimeFeatures.abs_mean(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"kurtosis\"] = TimeFeatures.kurtosis(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"skew\"] = TimeFeatures.skew(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"coef_var\"] = TimeFeatures.coef_var(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"zptp\"] = TimeFeatures.zptp(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"crest\"] = TimeFeatures.crest(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"impulse_factor\"] = TimeFeatures.impulse_factor(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"margin_factor\"] = TimeFeatures.margin_factor(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"shape_factor\"] = TimeFeatures.shape_factor(df[col])\n",
    "        obj_features[first_word+\"_\"+col+\"_\"+\"clearance_factor\"] = TimeFeatures.clearance_factor(df[col])\n",
    "\n",
    "    return obj_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    file_names = [\n",
    "        'frontside_internal_machine_signals.h5',\n",
    "        'frontside_external_sensor_signals.h5',\n",
    "        'backside_external_sensor_signals.h5',\n",
    "        'backside_internal_machine_signals.h5'\n",
    "    ]\n",
    "    \n",
    "    # Initialize an empty dataframe to store the features\n",
    "    features = pd.DataFrame()\n",
    "            \n",
    "    # Loop through each object in the data\n",
    "    for obj in data:\n",
    "        # Get the part_id and the anomaly\n",
    "        anomaly = obj[\"process_data\"][1][\"anomaly\"] # Assuming the anomaly is the same for all processes\n",
    "        \n",
    "        # Initialize a dictionary to store the features for this object\n",
    "        obj_features = []\n",
    "        obj_features = {\"anomaly\": int(anomaly)}\n",
    "        \n",
    "        # Loop through the process_data and load the data_paths\n",
    "        for process_data in obj[\"process_data\"]:\n",
    "            for data_paths  in process_data[\"data_paths\"]:\n",
    "                # if file path contain saw -> ignore\n",
    "                file_name = data_paths.split('/')[-1]\n",
    "                part_id = data_paths.split('/')[-2]\n",
    "                # check if file_name is points to the right path\n",
    "                if file_name not in file_names:\n",
    "                    continue\n",
    "\n",
    "                # Path to the file \n",
    "                data_paths = \"/data/cylinder_bottom_test/cnc_milling_machine/process_data/\" + part_id +\"/\"+ file_name\n",
    "                \n",
    "                # Check the file extension\n",
    "                if data_paths.endswith(\".h5\"):\n",
    "                    # # Load the h5 file\n",
    "                    if os.path.exists(data_paths):\n",
    "                        h5_file = h5py.File(data_paths, \"r\")\n",
    "                        # # Do something with the h5 file, e.g. extract some statistics\n",
    "                        obj_features = read_sensor(obj_features, data_paths)\n",
    "                        # # Close the h5 file\n",
    "                        h5_file.close()\n",
    "                    else:\n",
    "                        print(\"file does not exist: \")\n",
    "                        print(data_paths)\n",
    "                        \n",
    "                elif data_paths.endswith(\".csv\"):\n",
    "                    data_paths = \"data/\" + data_paths\n",
    "    \n",
    "    \n",
    "        # Append the obj_features dictionary to the features dataframe\n",
    "        if features.empty:\n",
    "            features = pd.DataFrame([obj_features])\n",
    "        else:\n",
    "            features = pd.concat([features, pd.DataFrame([obj_features])])\n",
    "    \n",
    "\n",
    "    print(f'Done ..')\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(test_df):\n",
    "    # Load the pre-trained imputer and scaler\n",
    "    imputer = joblib.load('imputer.joblib')\n",
    "    scaler = joblib.load('scalar.joblib')\n",
    "    \n",
    "    # Apply imputing to the test_tf dataframe\n",
    "    # test_df = imputer.transform(test_df)\n",
    "\n",
    "    # Set feature names for the scaler\n",
    "    scaler.feature_names_in_ = list(test_df.columns)\n",
    "    \n",
    "    # Apply scaling to the test_tf dataframe\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    return test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "features_test = read_data(meta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected features\n",
    "selected_features = pd.read_csv(\"selected_features_names.csv\")[\"column_names\"].tolist()\n",
    "\n",
    "# Keep only the selected features\n",
    "test_df = pd.concat([features_test[feature] for feature in selected_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# final output: feature matrix X\n",
    "X = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing the Model\n",
    "\n",
    "Import your machine learning model. In this notebook, you should not perform any training! This notebook is only used for the evaluation of the model on unseen test data.\n",
    "\n",
    "To export and import your already generated model, you can use the dump() and load() functions from the *joblib* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = joblib.load(\"random_forest_classifier.joblib\") # replace \"your model\" with your generated machine learning model\n",
    "\n",
    "# predict the anomaly label for the test data\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Result\n",
    "\n",
    "################################\n",
    "**Do not change the code in this section!**\n",
    "################################\n",
    "\n",
    "The evaluation is done using the f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score = 100 * f1_score(y_true, y_pred)\n",
    "\n",
    "print('The f1-score of the model on the test set is: {:.2f}%'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
